{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dadba4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [pmTheta]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ConSV/lib/python3.8/site-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36m_mp_sample\u001b[0;34m(draws, tune, step, chains, cores, chain, random_seed, start, progressbar, trace, model, callback, discard_tuned_samples, mp_ctx, pickle_backend, **kwargs)\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mdraw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m                     \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ConSV/lib/python3.8/site-packages/pymc3/parallel_sampling.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0mdraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessAdapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ConSV/lib/python3.8/site-packages/pymc3/parallel_sampling.py\u001b[0m in \u001b[0;36mrecv_draw\u001b[0;34m(processes, timeout)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mpipes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_msg_pipe\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ConSV/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ConSV/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-878d1758b1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;31m# Generate the sample kl divergences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# sample_kl_1, data_x1, data_y1, data_theta1 = player_1.sample_kl_divergences(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             sample_kl_1, data_theta1 = player_1.sample_kl_divergences(\n\u001b[0m\u001b[1;32m    270\u001b[0m                 [player_sample_sizes[0]], 1, posterior_sample_size, prior_mean, prior_cov, num_params, generate_fcn=p1_generate_fcn)\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/codebase/Convergence_SV/player_1.py\u001b[0m in \u001b[0;36msample_kl_divergences\u001b[0;34m(sample_size_range, num_samples, num_draws, prior_mean, prior_cov, num_params, generate_fcn)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# Sample from the posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 pmTrace = pm.sample(draws=num_draws, \n\u001b[0m\u001b[1;32m    240\u001b[0m                                     \u001b[0mcores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_cores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                                     \u001b[0mtune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuning_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ConSV/lib/python3.8/site-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(draws, step, init, n_init, start, trace, chain_idx, chains, cores, tune, progressbar, model, random_seed, discard_tuned_samples, compute_convergence_checks, callback, return_inferencedata, idata_kwargs, mp_ctx, pickle_backend, **kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0m_print_step_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mp_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msample_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparallel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0m_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not pickle model, sampling singlethreaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ConSV/lib/python3.8/site-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36m_mp_sample\u001b[0;34m(draws, tune, step, chains, cores, chain, random_seed, start, progressbar, trace, model, callback, discard_tuned_samples, mp_ctx, pickle_backend, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdiscard_tuned_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m             \u001b[0mtraces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_choose_chains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m             \u001b[0mtraces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_choose_chains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ConSV/lib/python3.8/site-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36m_choose_chains\u001b[0;34m(traces, tune)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not enough samples to build a trace.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "from os.path import join as oj\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "data_fir = '.data/California_housing'\n",
    "X, y = fetch_california_housing(data_home=data_fir, download_if_missing=True, return_X_y=True, as_frame=False)\n",
    "bunch = fetch_california_housing(data_home=data_fir, download_if_missing=True, return_X_y=False, as_frame=True)\n",
    "df = bunch['frame']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# print(df.columns)\n",
    "df = df.drop(columns=['Latitude', 'Longitude'])\n",
    "\n",
    "# print(df.describe())\n",
    "# print(df.head())\n",
    "\n",
    "y = df['MedHouseVal'].values\n",
    "X = df.drop(columns=['MedHouseVal']).values\n",
    "\n",
    "X = StandardScaler().fit_transform(X=X)\n",
    "y = minmax_scale(y)\n",
    "\n",
    "'''\n",
    "curr_best = float('inf')\n",
    "best_w = None\n",
    "best_lambda = None\n",
    "for reg_lambda in np.logspace(-15, 0, num=1000):\n",
    "    w = np.linalg.inv(X.T @ X + reg_lambda * np.identity(X.shape[1])) @ X.T @ y\n",
    "    risk = np.linalg.norm(w @ X.T - y)\n",
    "    if risk < curr_best:\n",
    "        best_w = w\n",
    "        curr_best = risk\n",
    "        best_lambda = reg_lambda\n",
    "        print('updating at: ', reg_lambda, curr_best)\n",
    "\n",
    "print(\"Best lambda: {}, empirical risk:{} , w:{}.\".format(best_lambda, curr_best, best_w))\n",
    "true_params = best_w\n",
    "best_lambda = best_lambda\n",
    "'''\n",
    "# true_params = [0.21031031,  0.04282435, -0.10801392,  0.09709814,  0.00516233, -0.01044389]\n",
    "# best_lambda = 2.47e-07\n",
    "\n",
    "\n",
    "import parameters_CaliH as pr\n",
    "import player_1 as player_1\n",
    "import player_2 as player_2\n",
    "import player_3 as player_3\n",
    "import player_4 as player_4\n",
    "\n",
    "from player_manager_LR_multi import sample_kl_divergences\n",
    "from utils import powerset\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from math import factorial as fac\n",
    "import numpy as np\n",
    "from scipy.stats import tvar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "reg_lambda = 1e-3\n",
    "from linear_regression_sampling import leverage_iid_sampling\n",
    "\n",
    "import random\n",
    "random.seed(7913)\n",
    "np.random.seed(7913)\n",
    "\n",
    "\n",
    "def impute_with_mean(X):\n",
    "    #Obtain mean of columns as you need, nanmean is convenient.\n",
    "    col_mean = np.nanmean(X, axis=0)\n",
    "\n",
    "    #Find indices that you need to replace\n",
    "    inds = np.where(np.isnan(X))\n",
    "\n",
    "    #Place column means in the indices. Align the arrays using take\n",
    "    X[inds] = np.take(col_mean, inds[1])\n",
    "    return X\n",
    "\n",
    "\n",
    "# P1_DATA_SIZE = 10000 # 100, 500, 2000\n",
    "# P1_LOCAL_SAMPLE_SIZE =  100 # 10, 100, 500\n",
    "# P1_LOCAL_SAMPLE = 'iid' # iid, lvg_iid\n",
    "\n",
    "P2_DATA_RATIO = 0.1  # 0.01, 0.1, 0.5\n",
    "P2_NAN_RATIO = 0.1 # 0.1, 0.2\n",
    "\n",
    "for P1_DATA_SIZE in [1000, 5000]:\n",
    "    for P1_LOCAL_SAMPLE_SIZE in [10, 500]:\n",
    "        # for P1_LOCAL_SAMPLE in ['iid' ,'lvg_iid']:\n",
    "        exp_dir = oj('multiplayer', 'CaliH', \"P1-{}_{}\".format(str(P1_DATA_SIZE), str(P1_LOCAL_SAMPLE_SIZE)) )\n",
    "\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "        log_file = open(oj(exp_dir, 'log')  ,\"w\")\n",
    "        sys.stdout = log_file\n",
    "\n",
    "        with open(oj(exp_dir, 'settings.txt'), 'w') as f:\n",
    "\n",
    "            f.write(\"Experiment Parameters: \\n\")\n",
    "\n",
    "            f.write(\"P1_DATA_SIZE =  \" + str(P1_DATA_SIZE) + '\\n')\n",
    "            f.write(\"P1_LOCAL_SAMPLE_SIZE =  \" + str(P1_LOCAL_SAMPLE_SIZE)+ '\\n')\n",
    "            # f.write(\"P1_LOCAL_SAMPLE =  \" + str(P1_LOCAL_SAMPLE)+ '\\n')\n",
    "            f.write(\"P2_DATA_RATIO =  \" + str(P2_DATA_RATIO)+ '\\n')\n",
    "            f.write(\"P2_NAN_RATIO =  \" + str(P2_NAN_RATIO)+ '\\n')\n",
    "            f.write(\"P1_DATA_SIZE =  \" + str(P1_DATA_SIZE)+ '\\n')\n",
    "\n",
    "            f.write(\"Algorithm Parameters: \\n\")\n",
    "\n",
    "            f.write(\"fisher_sample_size =  \" + str(pr.fisher_sample_size)+ '\\n')\n",
    "            f.write(\"posterior_sample_size =  \" + str(pr.posterior_sample_size)+ '\\n')\n",
    "            f.write(\"tuning_step =  \" + str(pr.tuning_step)+ '\\n')\n",
    "            f.write(\"num_params =  \" + str(pr.num_params)+ '\\n')\n",
    "            f.write(\"true_param =  \" + str(pr.true_param)+ '\\n')\n",
    "            f.write(\"best_lambda =  \" + str(pr.best_lambda)+ '\\n')\n",
    "\n",
    "            f.write(\"base_sample_size =  \" + str(pr.base_sample_size)+ '\\n')\n",
    "            f.write(\"base_sample_increment =  \" + str(pr.base_sample_increment)+ '\\n')\n",
    "            f.write(\"max_sample_increment =  \" + str(pr.max_sample_increment)+ '\\n')\n",
    "            f.write(\"max_iteration =  \" + str(pr.max_iteration)+ '\\n')\n",
    "            \n",
    "            f.write(\"sample_size_range =  \" + str(pr.sample_size_range)+ '\\n')\n",
    "            f.write(\"num_samples =  \" + str(pr.num_samples)+ '\\n')\n",
    "\n",
    "        posterior_sample_size = pr.posterior_sample_size\n",
    "        prior_mean = pr.prior_mean\n",
    "        prior_cov = pr.prior_cov\n",
    "        num_params = pr.num_params\n",
    "\n",
    "        base_sample_size = pr.base_sample_size\n",
    "        base_sample_increment = pr.base_sample_increment\n",
    "        max_sample_increment = pr.max_sample_increment\n",
    "        max_iteration = pr.max_iteration\n",
    "\n",
    " \n",
    "        indices_1 = np.random.choice(list(range(len(X))), P1_DATA_SIZE)\n",
    "        X_1, y_1 = X[indices_1], y[indices_1]\n",
    "\n",
    "        indices_3 = np.random.choice(list(range(len(X))), P1_DATA_SIZE)\n",
    "        X_3, y_3 = X[indices_3], y[indices_3]\n",
    "\n",
    "\n",
    "        # set player 2's data to be part of the full data with missing data\n",
    "        indices_2 = np.random.choice(list(range(len(X))), int(P2_DATA_RATIO * len(X)))\n",
    "        X_2, y_2 = X[indices_2], y[indices_2]\n",
    "\n",
    "        # -- create nans and impute nan with averages -- #\n",
    "        nan_count_2 = int(len(X_2) * P2_NAN_RATIO)  \n",
    "        index_nan_2 = np.random.choice(X_2.size, nan_count_2, replace=False)  \n",
    "        X_2.ravel()[index_nan_2] = np.nan\n",
    "        assert nan_count_2 == np.count_nonzero(np.isnan(X_2))\n",
    "\n",
    "        X_2 = impute_with_mean(X_2)\n",
    "        assert 0 == np.count_nonzero(np.isnan(X_2))\n",
    "\n",
    "\n",
    "        # set player 4's data to be part of the full data with missing data\n",
    "        indices_4 = np.random.choice(list(range(len(X))), int(P2_DATA_RATIO * len(X)))\n",
    "        X_4, y_4 = X[indices_4], y[indices_4]\n",
    "\n",
    "        # -- create nans and impute nan with averages -- #\n",
    "        nan_count_4 = int(len(X_4) * P2_NAN_RATIO)  \n",
    "        index_nan_4 = np.random.choice(X_4.size, nan_count_4, replace=False)  \n",
    "        X_4.ravel()[index_nan_4] = np.nan\n",
    "        assert nan_count_4 == np.count_nonzero(np.isnan(X_4))\n",
    "\n",
    "        X_4 = impute_with_mean(X_4)\n",
    "        assert 0 == np.count_nonzero(np.isnan(X_4))\n",
    "\n",
    "        def p1_generate_fcn(sample_size):\n",
    "            ''' can we use volume sampling to guarantee unbiased-ness? '''\n",
    "            sample_theta = np.zeros((sample_size, num_params))    \n",
    "            for i in range(sample_size):\n",
    "                # indices = fast_reg_vol_sampling(X_1, local_size, reg_lambda)\n",
    "                \n",
    "                # depends on the global variable of X_1\n",
    "                indices = leverage_iid_sampling(X_1, P1_LOCAL_SAMPLE_SIZE, reg_lambda)\n",
    "\n",
    "                x, y = X_1[indices], y_1[indices]\n",
    "\n",
    "                theta_hat = np.linalg.inv(x.T @ x + reg_lambda * np.identity(x.shape[1])) @ x.T @ y  \n",
    "                sample_theta[i] = theta_hat\n",
    "            return sample_theta\n",
    "\n",
    "\n",
    "        print(\"Global True param:\", pr.true_param)\n",
    "        beta = 1\n",
    "        \n",
    "        player_2.data_cov = np.diag(np.full(num_params, 2.5))\n",
    "\n",
    "        # Player 2 maintains a posterior of BLR trained on the data\n",
    "        player_2.data_cov_inv = np.linalg.inv(player_2.data_cov) + beta * (X_2.T @ X_2)\n",
    "        player_2.data_cov = np.linalg.inv(player_2.data_cov_inv)\n",
    "        player_2.data_mean = np.linalg.inv(X_2.T @ X_2 + reg_lambda * np.identity(X_2.shape[1])) @ X_2.T @ y_2\n",
    "\n",
    "        print(\"Player 2 maintained prior mean:\", player_2.data_mean)\n",
    "        print(\"Player 2 maintained prior cov:\", player_2.data_cov)\n",
    "\n",
    "        def p2_generate_fcn(sample_size):\n",
    "            return np.random.multivariate_normal(mean=player_2.data_mean, cov=player_2.data_cov, size=sample_size)\n",
    "\n",
    "        def p3_generate_fcn(sample_size):\n",
    "            ''' can we use volume sampling to guarantee unbiased-ness? '''\n",
    "            sample_theta = np.zeros((sample_size, num_params))    \n",
    "            for i in range(sample_size):\n",
    "                # indices = fast_reg_vol_sampling(X_1, local_size, reg_lambda)\n",
    "                \n",
    "                # depends on the global variable of X_1\n",
    "                indices = np.random.choice(np.arange(len(X_1)), size=P1_LOCAL_SAMPLE_SIZE)\n",
    "\n",
    "                x, y = X_1[indices], y_1[indices]\n",
    "\n",
    "                theta_hat = np.linalg.inv(x.T @ x + reg_lambda * np.identity(x.shape[1])) @ x.T @ y  \n",
    "                sample_theta[i] = theta_hat\n",
    "            return sample_theta\n",
    "\n",
    "        # Player 4 maintains a posterior of BLR trained on the data\n",
    "        player_4.data_cov = np.diag(np.full(num_params, 3.5))\n",
    "        \n",
    "        player_4.data_cov_inv = np.linalg.inv(player_4.data_cov) + beta * (X_4.T @ X_4)\n",
    "        player_4.data_cov = np.linalg.inv(player_4.data_cov_inv)\n",
    "        player_4.data_mean = np.linalg.inv(X_4.T @ X_4 + reg_lambda * np.identity(X_4.shape[1])) @ X_4.T @ y_4\n",
    "\n",
    "        print(\"Player 4 maintained prior mean:\", player_4.data_mean)\n",
    "        print(\"Player 4 maintained prior cov:\", player_4.data_cov)\n",
    "\n",
    "        def p4_generate_fcn(sample_size):\n",
    "            return np.random.multivariate_normal(mean=player_4.data_mean, cov=player_4.data_cov, size=sample_size)\n",
    "\n",
    "        N = 4\n",
    "        P_set = powerset(list(range(N)))\n",
    "\n",
    "        players = [player_1, player_2, player_3, player_4] # create a list of the player py files for calling player-specific custom functions\n",
    "\n",
    "\n",
    "        player_sample_sizes = [base_sample_size for _ in range(N)] # a list of N numbers\n",
    "\n",
    "        player_sample_size_lists = [[] for _ in range(N)] # a list of N lists, each of length=max_iterations \n",
    "\n",
    "        player_FI_lists = [[] for _ in range(N)] # a list of N lists, each of length=max_iterations \n",
    "\n",
    "        player_shapley_lists = [[] for _ in range(N)] # a list of N lists, each of length=max_iterations \n",
    "\n",
    "\n",
    "        for i in range(max_iteration):\n",
    "            # Progress\n",
    "            print(\"----------------- Iteration: {}/{} -----------------\".format(i + 1, max_iteration))\n",
    "            print(\"Sample size: {}\".format(player_sample_sizes))\n",
    "\n",
    "            # Record current sample sizes\n",
    "            for player_sample_size_list, player_sample_size in zip(player_sample_size_lists, player_sample_sizes):\n",
    "                player_sample_size_list.append(player_sample_size)\n",
    "            \n",
    "            \n",
    "            # Generate the sample kl divergences\n",
    "            # sample_kl_1, data_x1, data_y1, data_theta1 = player_1.sample_kl_divergences(\n",
    "            sample_kl_1, data_theta1 = player_1.sample_kl_divergences(\n",
    "                [player_sample_sizes[0]], 1, posterior_sample_size, prior_mean, prior_cov, num_params, generate_fcn=p1_generate_fcn)\n",
    "            \n",
    "            sample_kl_2, data_x2 = player_2.sample_kl_divergences(\n",
    "                [player_sample_sizes[1]], 1, posterior_sample_size, prior_mean, prior_cov, num_params, generate_fcn=p2_generate_fcn)\n",
    "            \n",
    "\n",
    "            sample_kl_3, data_theta3 = player_3.sample_kl_divergences(\n",
    "                [player_sample_sizes[2]], 1, posterior_sample_size, prior_mean, prior_cov, num_params, generate_fcn=p3_generate_fcn)\n",
    "            \n",
    "\n",
    "            sample_kl_4, data_x4 = player_4.sample_kl_divergences(\n",
    "                [player_sample_sizes[3]], 1, posterior_sample_size, prior_mean, prior_cov, num_params, generate_fcn=p4_generate_fcn)\n",
    "            \n",
    "\n",
    "            player_sample_kl_list = [sample_kl_1, sample_kl_2, sample_kl_3, sample_kl_4] # a list of N numbers for each iteration\n",
    "\n",
    "            player_index_data_dict = {0:[data_theta1], 1:[data_x2],\\\n",
    "                2:[data_theta3], 3:[data_x4]}\n",
    "\n",
    "\n",
    "            sample_kls = defaultdict(float) # a dict for later calculation of SV\n",
    "\n",
    "            # update the dict for individual player sample_kl\n",
    "            for player_index, player_sample_kl in enumerate(player_sample_kl_list):\n",
    "                 sample_kls[tuple([player_index])] = np.squeeze(np.asarray(player_sample_kl)) \n",
    "            \n",
    "\n",
    "            for subset in P_set:\n",
    "                print(subset)\n",
    "                if len(subset) <= 1:continue\n",
    "                \n",
    "                else:\n",
    "                    print(\"Executing the sample kl divergences for :\", subset)\n",
    "                    estimated_kl_values, post_mean = sample_kl_divergences(\n",
    "                        [sum(player_sample_sizes[index] for index in subset)] , 1, \n",
    "                        posterior_sample_size, prior_mean, prior_cov, num_params,            \n",
    "                        {index: player_index_data_dict[index] for index in subset}\n",
    "                        )\n",
    "                    print(\"Estimated kl values are: \", estimated_kl_values)\n",
    "                    sample_kls[tuple(subset)] = np.squeeze(np.asarray(estimated_kl_values))\n",
    "                    if len(subset) == N:\n",
    "                        # Get the current parameter estimate from all the players, used later for calculating FI\n",
    "                        estimated_param = post_mean\n",
    "           \n",
    "            # Compute Shapley values\n",
    "            sample_shapleys = [0 for _ in range(N)]\n",
    "\n",
    "            for index in range(N):\n",
    "                sample_shapleys[index] = 0\n",
    "                for subset in P_set:\n",
    "\n",
    "                    if index not in subset:\n",
    "                        subset_included = tuple(sorted(subset + [index]))\n",
    "\n",
    "                        C = len(subset) \n",
    "                        if C == 0:\n",
    "                            sample_shapleys[index] +=  (fac(C) * fac(N-C-1) / fac(N)) * \\\n",
    "                            sample_kls[subset_included]\n",
    "                        else:                    \n",
    "                            sample_shapleys[index] +=  (fac(C) * fac(N-C-1) / fac(N)) * \\\n",
    "                            (sample_kls[subset_included] - sample_kls[tuple(subset)])\n",
    "\n",
    "            for player_index, player_shapley_list in enumerate(player_shapley_lists):\n",
    "                player_shapley_list.append(sample_shapleys[player_index])\n",
    "                         \n",
    "\n",
    "            \n",
    "            theano.config.compute_test_value = 'ignore'\n",
    "\n",
    "            # Compute Fisher information matrix (determinants) \n",
    "            \n",
    "            player_FI_dets = []\n",
    "            \n",
    "            for player_index, player in enumerate(players):\n",
    "                \n",
    "                player_data = player_index_data_dict[player_index]\n",
    "                \n",
    "                # calling the custom estimate_FI for each player\n",
    "                emp_Fisher = player.estimate_FI(player_data, estimated_param, num_params)\n",
    "                \n",
    "                player_FI_det = np.linalg.det(emp_Fisher)\n",
    "                player_FI_dets.append(player_FI_det) # for comparison among players in this iteration\n",
    "                \n",
    "                player_FI_lists[player_index].append(player_FI_det) # for record keeping over iterations\n",
    "            \n",
    "            # Compute fair sharing rates\n",
    "\n",
    "            max_FI = max(player_FI_dets)\n",
    "            max_FI_sample_count = player_sample_sizes[player_FI_dets.index(max_FI)]\n",
    "            for i, (FI, sample_size) in enumerate(zip(player_FI_dets, player_sample_sizes)):\n",
    "                \n",
    "                if FI == max_FI:\n",
    "                    player_sample_sizes[i] += base_sample_increment\n",
    "\n",
    "                else:\n",
    "                    rate = np.power(max_FI / FI, 1.0 / num_params)\n",
    "                    target = round(max_FI_sample_count * rate)\n",
    "                    if sample_size < target:\n",
    "                        sample_size += min(target - sample_size, max_sample_increment)\n",
    "\n",
    "                    player_sample_sizes[i] = int(sample_size)\n",
    "\n",
    "                player_sample_sizes[i] = int(player_sample_sizes[i])\n",
    "\n",
    "\n",
    "        for player_index in range(N):\n",
    "\n",
    "            np.savetxt('cumulative_{}.txt'.format(str(player_index+1)), player_sample_size_lists[player_index])\n",
    "            \n",
    "            np.savetxt('shapley_fair_{}.txt'.format(str(player_index+1)), player_shapley_lists[player_index])\n",
    "            \n",
    "            np.savetxt('FI_det_{}.txt'.format(str(player_index+1)), player_FI_lists[player_index])\n",
    "\n",
    "        log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b827c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b9aeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4acb5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919d80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d882005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
